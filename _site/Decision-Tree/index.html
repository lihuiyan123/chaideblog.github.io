<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-CN" lang="zh-CN">
	<head>
		<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="author" content="CHAI" />
		<title>
			
				
				决策树
			
		</title>

		<link rel="shortcut icon" href="/image/favicon.ico" />
		<link href="/feed/" rel="alternate" title="CHAI" type="application/atom+xml" />
		<link rel="stylesheet" href="/media/css/style.css" />
		<link rel="stylesheet" href="/media/css/highlight.css" />
		<script type="text/javascript" src="/media/js/jquery-1.7.1.min.js"></script>
	</head>

	<body>
		<div id="container">
		<div id="main" role="main">

		<header>
			<h1>
				决策树
			</h1>
		</header>

		<nav>
			<span><a title="网站首页" class="" href="/">首页</a></span>
			<span><a title="文章分类" class="" href="/categories/">分类</a></span>
			<span><a title="标签索引" class="" href="/tags/">标签</a></span>
			<span><a title="留言交流" class="" href="/guestbook/">留言</a></span>
			<span><a title="关于站长" class="" href="/about/">关于</a></span>
        </nav>

        <article class="content">
			<section class="meta">
<span class="time">
    <time datetime="2016-12-04">2016-12-04</time>
</span>

 |
<span class="categories">
  分类
  
  <a href="/categories/#机器学习" title="机器学习">机器学习</a>&nbsp;
  
</span>


 |
<span class="tags">
  标签
  
  <a href="/tags/#《机器学习（周志华）》" title="《机器学习（周志华）》">《机器学习（周志华）》</a>&nbsp;
  
</span>

</section>
<section class="post">
<h2 id="决策树">决策树</h2>
<h3 id="41-基本流程">4.1 基本流程</h3>
<p>顾名思义，决策树是基于树结构来进行决策的，是一种常见的机器学习方法。</p>

<p>一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对于<strong>决策</strong>结果，其他每个结点对应于一个<strong>属性</strong>测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。从根结点到每个叶结点的<strong>路径</strong>对应了一个<strong>判定测试序列</strong>。</p>

<p>决策树学习的目的是产生一棵泛化能力强的决策树，基本流程遵循简单且直观的“分而治之”策略。</p>

<p>决策树学习基本算法：</p>

<p>输入：训练集<img src="http://latex.codecogs.com/png.latex?D=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})\}" />；属性集：<img src="http://latex.codecogs.com/png.latex?A=\{a_{1},a_{2},...,a_{d}\}" />。</p>

<p>过程：函数TreeGenerate(D, A)</p>
<ol>
  <li>生成结点node；</li>
  <li>if D中样本全属于同一类别C then</li>
  <li> 将node标记为C类叶结点；return</li>
  <li>end if</li>
  <li>if A=∅ OR D中样本在A上取值相同 then</li>
  <li> 将node标记为叶结点，其类别标记为D中样本数最多的类；return</li>
  <li>end if</li>
  <li>从A中选择最优划分属性<img src="http://latex.codecogs.com/png.latex?a_{*}" /></li>
  <li>for <img src="http://latex.codecogs.com/png.latex?a_{*}" />的每一个值<img src="http://latex.codecogs.com/png.latex?a^{v}_{*}" />do</li>
  <li> 为node生成一个分支；令<img src="http://latex.codecogs.com/png.latex?D_{v}" />表示D中在<img src="http://latex.codecogs.com/png.latex?a_{*}" />上取值为<img src="http://latex.codecogs.com/png.latex?a^{v}_{*}" />的样本子集；</li>
  <li> if <img src="http://latex.codecogs.com/png.latex?D_{v}" />为空 then</li>
  <li>  将分支结点标记为叶结点，其类别标记为D中样本最多的类；return</li>
  <li> else</li>
  <li>  以<img src="http://latex.codecogs.com/png.latex?TreeGenerate(D_{v}, A - \{a_{*}\}" />为分支结点</li>
  <li> end if</li>
  <li>end for</li>
</ol>

<p>输出：以node为根结点的一颗决策树</p>

<ul>
  <li>笔记：类别C指的是<img src="http://latex.codecogs.com/png.latex?y_{i}" /></li>
</ul>

<p>显然，有三种情况会导致递归返回：</p>
<ol>
  <li>当前结点包含的样本全属于同一类别，无需划分；</li>
  <li>当前属性集为空，或所有样本在所有属性上取值相同，无法划分；</li>
  <li>当前结点包含的样本集合为空，不能划分。</li>
</ol>

<p>第2种情况，我们将当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别；第3种情况，同样将当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别。</p>

<h3 id="42-划分选择">4.2 划分选择</h3>

<p>决策树算法最关键的是第8行，即如何选择最优划分属性。我们一般希望决策树的分支结点所包含的样本属于同一类别，即结点的“纯度”越来越高。</p>

<h4 id="421-信息增益">4.2.1 信息增益</h4>

<p><strong>信息熵</strong>是度量样本集合纯度最常用的一种指标。假设当前样本集合D中第k类样本所占的比例为<img src="http://latex.codecogs.com/png.latex?p_{k}(k=1,2,...,|y|)" />，则D的信息熵定义为</p>

<p><img src="http://latex.codecogs.com/png.latex?Ent(D)=-\sum_{k=1}^{|y|} p_{k}log_{2}p_{k}" /></p>

<p><img src="http://latex.codecogs.com/png.latex?Ent(D)" />的值越小，则D的<strong>纯度</strong>越高。</p>

<p>假设离散属性a有V个可能的取值<img src="http://latex.codecogs.com/png.latex?\{a^{1},a^{2},...,a^{V}\}" />，若使用a对样本集合D进行划分，产生V个分支结点，其中第v个分支结点包含D中所有属性上取值为<img src="http://latex.codecogs.com/png.latex?a^{v}" />的样本，记为<img src="http://latex.codecogs.com/png.latex?D^{v}" />。给分支结点赋权重<img src="http://latex.codecogs.com/png.latex?\frac{|D^{v}|}{|D|}" />，可以计算出用属性a对样本集D进行划分所得的<strong>信息增益</strong>。</p>

<p><img src="http://latex.codecogs.com/png.latex?Gain(D,a)=Ent(D)-\sum^{V}_{v=1} \frac{|D^{v}|}{|D|} Ent(D^{v})" /></p>

<p>一般，信息增益越大，意味着使用属性a来进行划分得到的<strong>纯度提升</strong>越大。因此，我们选择属性<img src="http://latex.codecogs.com/png.latex?a_{*}=\arg\ \max \limits_{a \in A} Gain(D,a)" />。著名的ID3算法就是通过信息增益来选择划分属性的。</p>

<h4 id="422-增益率">4.2.2 增益率</h4>

<p>信息增益准则对可取值数目较多的属性有偏好，为减少这种偏好带来的不利影响，著名的C4.5决策树算法采用<strong>增益率</strong>来选择最优属性。增益率定义为</p>

<p><img src="http://latex.codecogs.com/png.latex?Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}" /></p>

<p>其中，<img src="http://latex.codecogs.com/png.latex?IV(a)=-\sum^{V}_{v=1} \frac{|D^{v}|}{|D|}log_{2}\frac{|D^{v}|}{|D|}" />称为属性a的<strong>固有值</strong>。属性a的可能取值越多，<img src="http://latex.codecogs.com/png.latex?IV(a)" />越大。</p>

<p><strong>增益率对取值数目较小的属性有偏好。</strong></p>

<h4 id="423-基尼系数">4.2.3 基尼系数</h4>

<p>CART决策树通过<strong>基尼系数</strong>来选择划分属性，数据Ｄ的基尼值为：</p>

<p><img src="http://latex.codecogs.com/png.latex?Gini(D)=\sum^{|y|}_{k=1} \sum_{k^{'} \neq k} p_{k}p_{k^{'}} = 1 - \sum^{|y|}_{k=1} p^{2}_{k}" /></p>

<p>属性a的基尼指数定义为</p>

<p><img src="http://latex.codecogs.com/png.latex?Gini\_index(D,a)=\sum^{V}_{v=1} \frac{|D^{v}|}{|D|} Gini(D^{v})" /></p>

<p>于是，我们在属性集合A中，选择<img src="http://latex.codecogs.com/png.latex?a_{*}=\arg\ \max \limits_{a \in A} Gini\_index(D,a)" />。</p>

<h3 id="43-剪枝处理">4.3 剪枝处理</h3>

<p>剪枝是决策树学习算法对付<strong>过拟合</strong>的主要手段。决策树剪枝的基本策略有<strong>预剪枝</strong>和<strong>后剪枝</strong>。</p>

<p>预剪枝是在决策树生成过程中，对每个结点在划分前进行估计，当前结点的划分不能带来决策树泛化性能提升，则停止划分斌将当前结点标记为叶结点。后剪枝是从训练集生成的一颗完整的决策书，自底向上对非叶结点进行考察，若该结点对应的子树替换为叶结点能带来决策树泛化能力的提高，则将该子树替换为叶结点。</p>

<h4 id="431-预剪枝">4.3.1 预剪枝</h4>

<p>预剪枝的算法：</p>

<p>输入：训练集<img src="http://latex.codecogs.com/png.latex?D=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})\}" />；属性集：<img src="http://latex.codecogs.com/png.latex?A=\{a_{1},a_{2},...,a_{d}\}" />；验证集：<img src="http://latex.codecogs.com/png.latex?T=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\}" />。</p>

<p>过程：函数pre_TreeCut(D,T,A)</p>

<ol>
  <li>根据信息增益或增益率，选择属性<img src="http://latex.codecogs.com/png.latex?a_{*}" /></li>
  <li>if 划分后验证集精度提升 then</li>
  <li> 可以划分；对该结点调用<img src="http://latex.codecogs.com/png.latex?pre\_TreeCut(D,T,A - \{ a_{*} \})" /></li>
  <li>else</li>
  <li> 将该结点定义为叶结点；return</li>
  <li>end if</li>
</ol>

<p>预剪枝基于<strong>贪心</strong>本质禁止这些分支展开，给预剪枝决策树带来了<strong>欠拟合</strong>的风险。</p>

<h4 id="432-后剪枝">4.3.2 后剪枝</h4>

<p>后剪枝先从训练集中生成一颗完整的决策树。然后，自底向上依次判断剪枝是否会提高验证集精度。若提高，则进行剪枝；若下降，则不剪枝；若不变，根据奥卡姆剃刀准则，剪枝。</p>

<p>一般，后剪枝决策树的<strong>欠拟合</strong>风险很小，泛化性能优于预剪枝决策树；但是后剪枝的<strong>训练时间</strong>比预剪枝大得多。</p>

<h3 id="44-连续与缺失值">4.4 连续与缺失值</h3>

<h4 id="441-连续值处理">4.4.1 连续值处理</h4>

<p>对给定样本集D和连续属性a，假定a在D上出现了n个不同的取值，将这些取值从小到大排序，记为<img src="http://latex.codecogs.com/png.latex?\{a_{1},a_{2},...,a_{n}\}" />。基于划分点t可以将D分为子集<img src="http://latex.codecogs.com/png.latex?D_{t}^{-}" />和<img src="http://latex.codecogs.com/png.latex?D_{t}^{+}" />，其中<img src="http://latex.codecogs.com/png.latex?D_{t}^{-}" />包含属性a上不大于t的样本，<img src="http://latex.codecogs.com/png.latex?D_{t}^{+}" />包含属性a上大于t的样本。显然，对于相邻属性<img src="http://latex.codecogs.com/png.latex?a^{i}" />和<img src="http://latex.codecogs.com/png.latex?a^{i+1}" />来说，t在区间<img src="http://latex.codecogs.com/png.latex?[a^{i},a^{i+1}]" />取任意值所产生的划分结果相同。因此对连续属性a，考察包含n-1个元素的划分点集：</p>

<p><img src="http://latex.codecogs.com/png.latex?T_{a}=\{\frac{a^{i}+a^{i+1}}{2}|1 \leq i \leq n-1\}" /></p>

<p>信息增益为：</p>

<p><img src="http://latex.codecogs.com/png.latex?Gain(D,a)=\max \limits_{t \in T_{a}}\ Gain(D,a,t)=\max \limits_{t \in T_{a}}\ Ent(D)-\sum_{\lambda \in \{ -,+ \}} \frac{|D^{v}|}{|D|} Ent(D^{\lambda}_{t})" /></p>

<p>选择出最大的信息增益点后，找出对应的划分点。</p>

<h4 id="442-缺失值处理">4.4.2 缺失值处理</h4>

<p>利用有缺失属性值的训练样例来学习，需要解决两个问题：（1）如何在属性值缺失的情况下进行划分属性选择？（2）给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？</p>

<p>给定训练集D和属性a，令<img src="http://latex.codecogs.com/png.latex?\tilde{D}" />表示D在属性a上没有缺失值的样本子集。对问题（1），我们可以根据<img src="http://latex.codecogs.com/png.latex?\tilde{D}" />来判断属性a的优劣。</p>

<p>假定属性a有V个可取值<img src="http://latex.codecogs.com/png.latex?\{a^{1},a^{2},...,a^{V}\}" />，令<img src="http://latex.codecogs.com/png.latex?\tilde{D}^{v}" />表示<img src="http://latex.codecogs.com/png.latex?\tilde{D}" />中属性a上取值为<img src="http://latex.codecogs.com/png.latex?a^{v}" />的样本子集，<img src="http://latex.codecogs.com/png.latex?\tilde{D}_{k}" />表示<img src="http://latex.codecogs.com/png.latex?\tilde{D}" />中属于第k类<img src="http://latex.codecogs.com/png.latex?(k=1,2,...,|y|)" />的样本子集。假定我们为每个样本x赋予一个权重<img src="http://latex.codecogs.com/png.latex?\omega_{x}" />，并定义</p>

<p><img src="http://latex.codecogs.com/png.latex?\rho=\frac{\sum_{x \in \tilde{D}} \omega_{x}}{\sum_{x \in D} \omega_{x}}" /></p>

<p><img src="http://latex.codecogs.com/png.latex?\tilde{p}_{k}=\frac{\sum_{x \in \tilde{D}_{k}} \omega_{x}}{\sum_{x \in \tilde{D}} \omega_{x}}\ \ (1 \leq k \leq |y|)" /></p>

<p><img src="http://latex.codecogs.com/png.latex?\tilde{r}_{v}=\frac{\sum_{x \in \tilde{D}^{v}} \omega_{x}}{\sum_{x \in \tilde{D}} \omega_{x}}\ \ (1 \leq v \leq V)" /></p>

<p>因为，<img src="http://latex.codecogs.com/png.latex?\sum_{k=1}^{|y|}\tilde{p}_{k}=1" />和<img src="http://latex.codecogs.com/png.latex?\sum_{v=1}^{V}\tilde{r}_{v}=1" />；可知：</p>

<p><img src="http://latex.codecogs.com/png.latex?Gain(D,a)=\rho \times Gain(\tilde{D},a)=\rho \times \bigg(Ent(\tilde{D})-\sum_{v=1}^{V} Ent(\tilde{D}^v)\bigg)" /></p>

<p><img src="http://latex.codecogs.com/png.latex?Ent(\tilde{D})=-\sum_{k=1}^{|y|} \tilde{p}_{k}log_{2}\tilde{p}_{k}" /></p>

<p>对于问题（2），若样本x在划分属性a上的取值已知，则将x划入相应的子结点，且样本权重保持为<img src="http://latex.codecogs.com/png.latex?\omega_{x}" />。若样本x在划分属性a上的取值未知，则将x同时划入所有子结点，且样本权重与属性值<img src="http://latex.codecogs.com/png.latex?a^{v}" />对应的子结点中调整为<img src="http://latex.codecogs.com/png.latex?\tilde{r}_{v} \cdot \omega_{x}" />。</p>

<h3 id="45-多变量决策树">4.5 多变量决策树</h3>

<p>决策树所形成的分类边界有一个明显的特点：轴平行，即它的分类边界有若干个与坐标轴平行的分段组成。在学习任务的真实分类边界复杂时，决策树会十分复杂，时间开销会很大。</p>

<p>多变量决策树可以大大简化决策树模型。例如斜划分的多变量决策树中，非叶结点不再是针对有个属性，而是对属性的线性组合，形成一个形如<img src="http://latex.codecogs.com/png.latex?\sum_{i=1}^{d} \omega_{i}a_{i}=t" />的线性分类器。</p>

</section>
<section align="right">
<br/>
<span>
	<a  href="/Python-Learn-chapter9/" class="pageNav"  >上一篇</a>
	&nbsp;&nbsp;&nbsp;
	<a   class="pageNavInvalid"  >下一篇</a>
</span>
</section>

        </article>
		</div>
		</div>
	</body>
</html>
