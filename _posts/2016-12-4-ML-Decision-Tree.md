---
layout: post
title:  "机器学习笔记——决策树"
categories: 机器学习
tags: "Decision Tree"
excerpt: 决策树是基于树结构来进行决策的，是一种常见的机器学习方法。
---

## 浅谈决策树在数据挖掘中的应用

数据挖掘(KDD)可以产生五种基本类型的信息：(1)关联信息，若干个事情相关联的信息；(2)聚类信息，对数据进行聚类；(3)分类信息，进行分类的特征描述；(4)偏差信息，反应异常情况；(5)预测信息。

数据挖掘中用于分类的算法很多，如决策树、贝叶斯分类、规则推理、遗传算法和神经网络。其中决策树的应用广泛，原因是：(1)决策树算法的复杂度小，速度快；(2)抗噪声能力强；(3)可伸缩性强，即可用于小数据集，也可用于海量数据集。

## 建树过程

决策树算法的研究方向：(1)提高效率；(2)适应多数据类型、容噪；(3)可扩展性；(4)与其他技术相结合，如神经网络、模糊集理论、遗传算法。

### 概念学习系统(CLS)

CLS的描述如下：

1. 生成一颗空决策树和一张训练样本属性表；
2. if&ensp;样本都属于同一类
3. &emsp;生成结点T，结束；
4. else
5. &emsp;根据某种策略，从属性表中选出属性A，并产生结点A
6. &emsp;假定A的取值为$$\{v_{1},v_{2},...,v_{m}\}$$，根据A的取值，将T分为M个子集$$\{T_{1},T_{2},...,T_{m}\}$$；
7. &emsp;从训练样本属性表中删除属性A；
8. 转步骤2，递归调用。

### 迭代分类器(ID3)

ID3算法是决策树算法的代表。它采用分治策略，在决策树各级结点选择属性时，采用信息增益作为标准。

一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对于**决策**结果，其他每个结点对应于一个**属性**测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。从根结点到每个叶结点的**路径**对应了一个**判定测试序列**。

ID3基本算法：

输入：训练集$$D=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})\}$$；属性集：$$A=\{a_{1},a_{2},...,a_{d}\}$$。

过程：函数ID3(D,A)

1. 生成结点node；
2. if D中样本全属于同一类别C then
3. &emsp;将node标记为C类叶结点；return
4. end if
5. if A=∅ OR D中样本在A上取值相同 then
6. &emsp;将node标记为叶结点，其类别标记为D中样本数最多的类；return
7. end if
8. 从A中选择最优划分属性$$a_{*}$$
9. for $$a_{*}$$的每一个值$$a^{v}_{*}$$do
10. &emsp;为node生成一个分支；令$$D_{v}$$表示D中在$$a_{*}$$上取值为$$a^{v}_{*}$$的样本子集；
11. &emsp;if $$D_{v}$$为空 then
12. &emsp;&emsp;将分支结点标记为叶结点，其类别标记为D中样本最多的类；return
13. &emsp;else
14. &emsp;&emsp;以$$ID3(D_{v},A - \{a_{*}\}$$为分支结点
15. &emsp;end if
16. end for

输出：以node为根结点的一颗决策树

* 笔记：类别C指的是$$y_{i}$$

显然，有三种情况会导致递归返回：

1. 当前结点包含的样本全属于同一类别，无需划分；
2. 当前属性集为空，或所有样本在所有属性上取值相同，无法划分；
3. 当前结点包含的样本集合为空，不能划分。

第2种情况，我们将当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别；第3种情况，同样将当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别。

决策树算法最关键的是第8行，即如何选择最优划分属性。我们一般希望决策树的分支结点所包含的样本属于同一类别，即结点的“纯度”越来越高。

**信息熵**是度量样本集合纯度最常用的一种指标。假设当前样本集合D中第k类样本所占的比例为
$$p_{k}(k=1,2,...,|y|)$$
，则D的信息熵定义为

$$Ent(D)=-\sum_{k=1}^{|y|} p_{k}log_{2}p_{k}$$

$$Ent(D)$$的值越小，则D的**纯度**越高。

假设离散属性a有V个可能的取值$$\{a^{1},a^{2},...,a^{V}\}$$，若使用a对样本集合D进行划分，产生V个分支结点，其中第v个分支结点包含D中所有属性上取值为$$a^{v}$$的样本，记为$$D^{v}$$。给分支结点赋权重
$$\frac{|D^{v}|}{|D|}$$
，可以计算出用属性a对样本集D进行划分所得的**信息增益**。

$$Gain(D,a)=Ent(D)-\sum^{V}_{v=1} \frac{|D^{v}|}{|D|} Ent(D^{v})$$

一般，信息增益越大，意味着使用属性a来进行划分得到的**纯度提升**越大。因此，我们选择属性$$a_{*}=\arg\ \max \limits_{a \in A} Gain(D,a)$$。著名的ID3算法就是通过信息增益来选择划分属性的。

### ACLS：ID3+CLS

主要改进：运行属性取任意的整数值，极大的扩展了决策树算法的应用范围，使决策树可以处理一些比较复杂的任务，比如图像处理。

### CART

采用基尼系数作为选择属性的依据，最终生成二叉树。再利用重采样技术进行误差估计和基于最小代价复杂性的树剪枝。

由于生成决策树的训练集一般都存在噪声数据，噪声数据会造成决策树过于复杂，决策树剪枝方法的出现极大改善了性能。剪枝方法有：最小错误率剪枝(MEP)、临界值剪枝(CVP)、减少错误剪枝(REP)。

CART决策树通过**基尼系数**来选择划分属性，数据Ｄ的基尼值为：

$$Gini(D)=\sum^{|y|}_{k=1}\sum_{k^{'}\neq k} p_{k}p_{k^{'}}=1-\sum^{|y|}_{k=1}p^{2}_{k}$$

属性a的基尼指数定义为

$$Gini\_index(D,a)=\sum^{V}_{v=1} \frac{|D^{v}|}{|D|}Gini(D^{v})$$

于是，我们在属性集合A中，选择
$$a_{*}=\arg\ \max \limits_{a \in A} Gini\_index(D,a)$$
。

### RELIEF

考虑“邻居”实例，把数据集中的局部信息引入到决策树算法中。局部信息的优势在于它能够在其他属性的背景下评估每一个属性，此前的决策树算法都只能单独的评估每一个属性，忽略的属性间的关联。

### C4.5

采用信息增益率来选择属性，同时可以处理连续值属性的数据。还可以用于增量式学习，随着数据量的增加动态地调整决策树。

#### 信息增益率

信息增益准则对可取值数目较多的属性有偏好，为减少这种偏好带来的不利影响，著名的C4.5决策树算法采用**增益率**来选择最优属性。增益率定义为

$$Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$

其中，
$$IV(a)=-\sum^{V}_{v=1}\frac{|D^{v}|}{|D|}log_{2}\frac{|D^{v}|}{|D|}$$
称为属性a的**固有值**。属性a的可能取值越多，$$IV(a)$$越大。

**增益率对取值数目较小的属性有偏好。**

对给定样本集D和连续属性a，假定a在D上出现了n个不同的取值，将这些取值从小到大排序，记为$$\{a_{1},a_{2},...,a_{n}\}$$。基于划分点t可以将D分为子集$$D_{t}^{-}$$和$$D_{t}^{+}$$，其中$$D_{t}^{-}$$包含属性a上不大于t的样本，$$D_{t}^{+}$$包含属性a上大于t的样本。显然，对于相邻属性$$a^{i}$$和$$a^{i+1}$$来说，t在区间$$[a^{i},a^{i+1}]$$取任意值所产生的划分结果相同。因此对连续属性a，考察包含n-1个元素的划分点集：

$$T_{a}=\{\frac{a^{i}+a^{i+1}}{2}|1 \leq i \leq n-1\}$$

信息增益为：

$$Gain(D,a)=\max \limits_{t \in T_{a}}\ Gain(D,a,t)=\max \limits_{t \in T_{a}}\ Ent(D)-\sum_{\lambda \in \{ -,+ \}} \frac{|D^{v}|}{|D|} Ent(D^{\lambda}_{t})$$

选择出最大的信息增益点后，找出对应的划分点。

#### 缺失值处理

利用有缺失属性值的训练样例来学习，需要解决两个问题：（1）如何在属性值缺失的情况下进行划分属性选择？（2）给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？

给定训练集D和属性a，令$$\tilde{D}$$表示D在属性a上没有缺失值的样本子集。对问题（1），我们可以根据$$\tilde{D}$$来判断属性a的优劣。

假定属性a有V个可取值$$\{a^{1},a^{2},...,a^{V}\}$$，令$$\tilde{D}^{v}$$表示$$\tilde{D}$$中属性a上取值为$$a^{v}$$的样本子集，$$\tilde{D}_{k}$$
表示$$\tilde{D}$$
中属于第k类
$$(k=1,2,...,|y|)$$
的样本子集。假定我们为每个样本x赋予一个权重
$$\omega_{x}$$
，并定义

$$\rho=\frac{\sum_{x \in \tilde{D}} \omega_{x}}{\sum_{x \in D} \omega_{x}}$$

$$\tilde{p}_{k}=\frac{\sum_{x \in \tilde{D}_{k}} \omega_{x}}{\sum_{x \in \tilde{D}} \omega_{x}}\ \ (1 \leq k \leq |y|)$$

$$\tilde{r}_{v}=\frac{\sum_{x \in \tilde{D}^{v}} \omega_{x}}{\sum_{x \in \tilde{D}} \omega_{x}}\ \ (1 \leq v \leq V)$$

因为，
$$\sum_{k=1}^{|y|}\tilde{p}_{k}=1$$
和
$$\sum_{v=1}^{V}\tilde{r}_{v}=1$$
；可知：

$$Gain(D,a)=\rho \times Gain(\tilde{D},a)=\rho \times \bigg(Ent(\tilde{D})-\sum_{v=1}^{V} Ent(\tilde{D}^v)\bigg)$$

$$Ent(\tilde{D})=-\sum_{k=1}^{|y|} \tilde{p}_{k}log_{2}\tilde{p}_{k}$$

对于问题（2），若样本x在划分属性a上的取值已知，则将x划入相应的子结点，且样本权重保持为$$\omega_{x}$$。若样本x在划分属性a上的取值未知，则将x同时划入所有子结点，且样本权重与属性值$$a^{v}$$对应的子结点中调整为$$\tilde{r}_{v} \cdot \omega_{x}$$。

### SLIQ

高速、可伸缩、有监督的寻找学习。针对数据量远大于内存的情况，采用了类表、属性表和类直方图三种数据结构，利用换入换出策略处理大数据量。

### SPRINT

可伸缩、并行、归纳决策树。可以避免内存空间的限制，利用多个并行处理器构造一个稳定的、分类准确率很高的决策树。具有很好的可伸缩性，扩容性。但是结点分割处理过程复杂和储存复杂。

### PUBLIC

将建树和修剪相结合。主要思想：在决策树建立时，计算每个结点的目标函数值，估计该结点在未来是否会被删除。

### 雨林分类法

针对大数据量，快速构造决策树的分类框架。核心思想：根据每次计算时可用的内存空间，合理调整每次计算的数据集大小，尽量利用内存资源。

### EC4.5

采用二分搜索代替线性搜索，同时提出多种寻找连续值的局部阈值的策略。

### 软决策树

综合决策树的生成和修剪来决定本身的结构，并利用重修和磨合来提高归纳能力。

### 多变量决策树

决策树所形成的分类边界有一个明显的特点：轴平行，即它的分类边界有若干个与坐标轴平行的分段组成。在学习任务的真实分类边界复杂时，决策树会十分复杂，时间开销会很大。

多变量决策树可以大大简化决策树模型。例如斜划分的多变量决策树中，非叶结点不再是针对有个属性，而是对属性的线性组合，形成一个形如$$\sum_{i=1}^{d} \omega_{i}a_{i}=t$$的线性分类器。

## 剪枝处理

剪枝是决策树学习算法对付**过拟合**的主要手段。决策树剪枝的基本策略有**预剪枝**和**后剪枝**。

预剪枝是在决策树生成过程中，对每个结点在划分前进行估计，当前结点的划分不能带来决策树泛化性能提升，则停止划分斌将当前结点标记为叶结点。后剪枝是从训练集生成的一颗完整的决策书，自底向上对非叶结点进行考察，若该结点对应的子树替换为叶结点能带来决策树泛化能力的提高，则将该子树替换为叶结点。

### 预剪枝

预剪枝的算法：

输入：训练集$$D=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})\}$$；属性集：$$A=\{a_{1},a_{2},...,a_{d}\}$$；验证集：$$T=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\}$$。

过程：函数pre_TreeCut(D,T,A)

1. 根据信息增益或增益率，选择属性$$a_{*}$$

2. if 划分后验证集精度提升 then

3. &emsp;可以划分；对该结点调用$$pre\_TreeCut(D,T,A - \{ a_{*} \})$$

4. else

5. &emsp;将该结点定义为叶结点；return

6. end if

预剪枝基于**贪心**本质禁止这些分支展开，给预剪枝决策树带来了**欠拟合**的风险。预剪枝的缺点是不能处理数量较小的特殊情况实例；还有视野效果问题，就是在相同的标准下，当前的扩展不能满足要求，但进一步的扩展可以满足要求。

### 后剪枝

一般，后剪枝决策树的**欠拟合**风险很小，泛化性能优于预剪枝决策树；但是后剪枝的**训练时间**比预剪枝大得多。

#### REP方法

自底而上，对于树T的每一个子树S，使它成为叶结点，生成一颗新树。在测试集上，新树可以得到一个较小或相等的分类错误，则S删除。

REP方法是线性的，但偏向过度修剪，当训练数据集较小，不建议使用。

#### PEP方法

为了克服REP方法需要独立剪枝数据集的缺点设计的。

假设训练集生成树T，叶结点的实例个数为$$n(T)$$，其中错误分类个数$$e(T)$$，误差率$$r(T)=\frac{e(T)}{n(T)}$$。对误差估计增加了连续性校正$$r^{'}(T)=\frac{e(T)+\frac{1}{2}}{n(T)}$$。

设$$T_{s}$$为树T的子树，其叶结点的个数为$$L($$T_{s}$$)$$；s是树$$T_{s}$$替换后的叶结点。$$T_{s}$$的分类误差为：

$$r^{'}(T_{s})=\frac{\sum \limits{T_{s}} [e(T_{s})+\frac{1}{2}]}{\sum \limits{T_{s}} n(T_{s})} = \frac{\sum e(T_{s})+\frac{L(T_{s})}{2}}{\sum n(T_{s})}$$

令$$e^{'}(T_{s})=\sum e(T_{s}) + \frac{L(T_{s})}{2}$$

一般，结点s别叶结点替换的条件是：子树S的误差率大于替换后结点s的误差率。

削弱对错误率的限制，修改为：

$$e^{'}(s) \leq e^{'}(T_{s}) + SE[e^{'}(T_{s})]$$

其中，$$SE[e^{'}(T_{s})]$$称为标准误差，定义：

$$SE[e^{'}(T_{s})] = \sqrt{\frac{e^{'}(T_{s})[n(s)-e^{'}(T_{s})]}{n(s)}}$$

如果上式成立，则子树$$T_{s}$$应被修剪掉。

#### MEP方法

假设训练集的类别总数为k，对于书中的当前非叶子结点t，所包含的样本数$$N(t)$$，属于主导类i的样本个数为$$N_{i}(t)$$，不属于主导类的样本数为$$E(t)$$，显然$$E(t)=N(t)-N_{i}(t)$$。结点t中某一样本实例属于类i的期望概率为

$$P_{i}(t)=\frac{N_{i}(t)+P_{ki}*m}{N(t)+m}$$

其中，$$P_{ki}$$表示第i类的先验概率，m是参数，用来设置先验概率在评估$$P_{i}(t)$$中的权值。

如果当前结点t标示为类i的叶结点，那么分类误差期望概率为：

$$EER(t)=min{1-P_{i}(t)}=min{\frac{N(t)-N_{i}(t)+m \cdot (1-P_{ki})}{N(t)+m}}$$

对于非叶结点t，假设t的子结点$$t_{1},t_{2},...,t_{m}$$，首先计算该结点t的误差，称为静态误差$$STE(t)$$，然后计算这m个分支的误差，并且加权求和，权值为每个分支拥有的训练样本比例。我们称加权后的误差为回溯误差$$DYE(t)$$。如果$$STE(t) \leq DYE(t)$$，则对结点t的子树进行修剪。
