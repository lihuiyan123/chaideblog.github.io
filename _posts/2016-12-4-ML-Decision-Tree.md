---
layout: post
title:  "机器学习笔记——决策树"
categories: 机器学习
tags: 《机器学习（周志华）》
excerpt: 决策树是基于树结构来进行决策的，是一种常见的机器学习方法。
---

## 决策树
### 4.1 基本流程
顾名思义，决策树是基于树结构来进行决策的，是一种常见的机器学习方法。

一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对于**决策**结果，其他每个结点对应于一个**属性**测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。从根结点到每个叶结点的**路径**对应了一个**判定测试序列**。

决策树学习的目的是产生一棵泛化能力强的决策树，基本流程遵循简单且直观的“分而治之”策略。

决策树学习基本算法：

输入：训练集$$D=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})\}$$；属性集：$$A=\{a_{1},a_{2},...,a_{d}\}$$。

过程：函数TreeGenerate(D, A)
1. 生成结点node；
2. if D中样本全属于同一类别C then
3. &emsp;将node标记为C类叶结点；return
4. end if
5. if A=∅ OR D中样本在A上取值相同 then
6. &emsp;将node标记为叶结点，其类别标记为D中样本数最多的类；return
7. end if
8. 从A中选择最优划分属性$$a_{*}$$
9. for $$a_{*}$$的每一个值$$a^{v}_{*}$$do
10. &emsp;为node生成一个分支；令$$D_{v}$$表示D中在$$a_{*}$$上取值为$$a^{v}_{*}$$的样本子集；
11. &emsp;if $$D_{v}$$为空 then
12. &emsp;&emsp;将分支结点标记为叶结点，其类别标记为D中样本最多的类；return
13. &emsp;else
14. &emsp;&emsp;以$$TreeGenerate(D_{v}, A - \{a_{*}\}$$为分支结点
15. &emsp;end if
16. end for

输出：以node为根结点的一颗决策树

* 笔记：类别C指的是$$y_{i}$$

显然，有三种情况会导致递归返回：
1. 当前结点包含的样本全属于同一类别，无需划分；
2. 当前属性集为空，或所有样本在所有属性上取值相同，无法划分；
3. 当前结点包含的样本集合为空，不能划分。

第2种情况，我们将当前结点标记为叶结点，并将其类别设定为该结点所含样本最多的类别；第3种情况，同样将当前结点标记为叶结点，但将其类别设定为其父结点所含样本最多的类别。

### 4.2 划分选择

决策树算法最关键的是第8行，即如何选择最优划分属性。我们一般希望决策树的分支结点所包含的样本属于同一类别，即结点的“纯度”越来越高。

#### 4.2.1 信息增益

**信息熵**是度量样本集合纯度最常用的一种指标。假设当前样本集合D中第k类样本所占的比例为
$$p_{k}(k=1,2,...,|y|)$$
，则D的信息熵定义为

$$Ent(D)=-\sum_{k=1}^{|y|} p_{k}log_{2}p_{k}$$

$$Ent(D)$$的值越小，则D的**纯度**越高。

假设离散属性a有V个可能的取值$$\{a^{1},a^{2},...,a^{V}\}$$，若使用a对样本集合D进行划分，产生V个分支结点，其中第v个分支结点包含D中所有属性上取值为$$a^{v}$$的样本，记为$$D^{v}$$。给分支结点赋权重
$$\frac{|D^{v}|}{|D|}$$
，可以计算出用属性a对样本集D进行划分所得的**信息增益**。

$$Gain(D,a)=Ent(D)-\sum^{V}_{v=1} \frac{|D^{v}|}{|D|} Ent(D^{v})$$

一般，信息增益越大，意味着使用属性a来进行划分得到的**纯度提升**越大。因此，我们选择属性$$a_{*}=\arg\ \max \limits_{a \in A} Gain(D,a)$$。著名的ID3算法就是通过信息增益来选择划分属性的。

#### 4.2.2 增益率

信息增益准则对可取值数目较多的属性有偏好，为减少这种偏好带来的不利影响，著名的C4.5决策树算法采用**增益率**来选择最优属性。增益率定义为

$$Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$

其中，
$$IV(a)=-\sum^{V}_{v=1}\frac{|D^{v}|}{|D|}log_{2}\frac{|D^{v}|}{|D|}$$
称为属性a的**固有值**。属性a的可能取值越多，$$IV(a)$$越大。

**增益率对取值数目较小的属性有偏好。**

#### 4.2.3 基尼系数

CART决策树通过**基尼系数**来选择划分属性，数据Ｄ的基尼值为：

$$Gini(D)=\sum^{|y|}_{k=1}\sum_{k^{'}\neq k} p_{k}p_{k^{'}}=1-\sum^{|y|}_{k=1}p^{2}_{k}$$

属性a的基尼指数定义为

$$Gini\_index(D,a)=\sum^{V}_{v=1} \frac{|D^{v}|}{|D|}Gini(D^{v})$$

于是，我们在属性集合A中，选择
$$a_{*}=\arg\ \max \limits_{a \in A} Gini\_index(D,a)$$
。

### 4.3 剪枝处理

剪枝是决策树学习算法对付**过拟合**的主要手段。决策树剪枝的基本策略有**预剪枝**和**后剪枝**。

预剪枝是在决策树生成过程中，对每个结点在划分前进行估计，当前结点的划分不能带来决策树泛化性能提升，则停止划分斌将当前结点标记为叶结点。后剪枝是从训练集生成的一颗完整的决策书，自底向上对非叶结点进行考察，若该结点对应的子树替换为叶结点能带来决策树泛化能力的提高，则将该子树替换为叶结点。

#### 4.3.1 预剪枝

预剪枝的算法：

输入：训练集$$D=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})\}$$；属性集：$$A=\{a_{1},a_{2},...,a_{d}\}$$；验证集：$$T=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\}$$。

过程：函数pre_TreeCut(D,T,A)

1. 根据信息增益或增益率，选择属性$$a_{*}$$
2. if 划分后验证集精度提升 then
3. &emsp;可以划分；对该结点调用$$pre\_TreeCut(D,T,A - \{ a_{*} \})$$
4. else
5. &emsp;将该结点定义为叶结点；return
6. end if


预剪枝基于**贪心**本质禁止这些分支展开，给预剪枝决策树带来了**欠拟合**的风险。

#### 4.3.2 后剪枝

后剪枝先从训练集中生成一颗完整的决策树。然后，自底向上依次判断剪枝是否会提高验证集精度。若提高，则进行剪枝；若下降，则不剪枝；若不变，根据奥卡姆剃刀准则，剪枝。

一般，后剪枝决策树的**欠拟合**风险很小，泛化性能优于预剪枝决策树；但是后剪枝的**训练时间**比预剪枝大得多。

### 4.4 连续与缺失值

#### 4.4.1 连续值处理

对给定样本集D和连续属性a，假定a在D上出现了n个不同的取值，将这些取值从小到大排序，记为$$\{a_{1},a_{2},...,a_{n}\}$$。基于划分点t可以将D分为子集$$D_{t}^{-}$$和$$D_{t}^{+}$$，其中$$D_{t}^{-}$$包含属性a上不大于t的样本，$$D_{t}^{+}$$包含属性a上大于t的样本。显然，对于相邻属性$$a^{i}$$和$$a^{i+1}$$来说，t在区间$$[a^{i},a^{i+1}]$$取任意值所产生的划分结果相同。因此对连续属性a，考察包含n-1个元素的划分点集：

$$T_{a}=\{\frac{a^{i}+a^{i+1}}{2}|1 \leq i \leq n-1\}$$

信息增益为：

$$Gain(D,a)=\max \limits_{t \in T_{a}}\ Gain(D,a,t)=\max \limits_{t \in T_{a}}\ Ent(D)-\sum_{\lambda \in \{ -,+ \}} \frac{|D^{v}|}{|D|} Ent(D^{\lambda}_{t})$$

选择出最大的信息增益点后，找出对应的划分点。

#### 4.4.2 缺失值处理

利用有缺失属性值的训练样例来学习，需要解决两个问题：（1）如何在属性值缺失的情况下进行划分属性选择？（2）给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？

给定训练集D和属性a，令$$\tilde{D}$$表示D在属性a上没有缺失值的样本子集。对问题（1），我们可以根据$$\tilde{D}$$来判断属性a的优劣。

假定属性a有V个可取值$$\{a^{1},a^{2},...,a^{V}\}$$，令$$\tilde{D}^{v}$$表示$$\tilde{D}$$中属性a上取值为$$a^{v}$$的样本子集，$$\tilde{D}_{k}$$
表示$$\tilde{D}$$
中属于第k类
$$(k=1,2,...,|y|)$$
的样本子集。假定我们为每个样本x赋予一个权重
$$\omega_{x}$$
，并定义

$$\rho=\frac{\sum_{x \in \tilde{D}} \omega_{x}}{\sum_{x \in D} \omega_{x}}$$

$$\tilde{p}_{k}=\frac{\sum_{x \in \tilde{D}_{k}} \omega_{x}}{\sum_{x \in \tilde{D}} \omega_{x}}\ \ (1 \leq k \leq |y|)$$

$$\tilde{r}_{v}=\frac{\sum_{x \in \tilde{D}^{v}} \omega_{x}}{\sum_{x \in \tilde{D}} \omega_{x}}\ \ (1 \leq v \leq V)$$

因为，
$$\sum_{k=1}^{|y|}\tilde{p}_{k}=1$$
和
$$\sum_{v=1}^{V}\tilde{r}_{v}=1$$
；可知：

$$Gain(D,a)=\rho \times Gain(\tilde{D},a)=\rho \times \bigg(Ent(\tilde{D})-\sum_{v=1}^{V} Ent(\tilde{D}^v)\bigg)$$

$$Ent(\tilde{D})=-\sum_{k=1}^{|y|} \tilde{p}_{k}log_{2}\tilde{p}_{k}$$

对于问题（2），若样本x在划分属性a上的取值已知，则将x划入相应的子结点，且样本权重保持为$$\omega_{x}$$。若样本x在划分属性a上的取值未知，则将x同时划入所有子结点，且样本权重与属性值$$a^{v}$$对应的子结点中调整为$$\tilde{r}_{v} \cdot \omega_{x}$$。

### 4.5 多变量决策树

决策树所形成的分类边界有一个明显的特点：轴平行，即它的分类边界有若干个与坐标轴平行的分段组成。在学习任务的真实分类边界复杂时，决策树会十分复杂，时间开销会很大。

多变量决策树可以大大简化决策树模型。例如斜划分的多变量决策树中，非叶结点不再是针对有个属性，而是对属性的线性组合，形成一个形如$$\sum_{i=1}^{d} \omega_{i}a_{i}=t$$的线性分类器。
